{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "controlling-calculator",
   "metadata": {},
   "source": [
    "# NLI Example\n",
    "\n",
    "Based on https://talman.io/nli/pytorch/demo/2020/12/11/natural-language-inference-with-pytorch-and-transformers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "excellent-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "robust-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW, logging\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "derived-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_nli (/home/jmperez/.cache/huggingface/datasets/multi_nli/plain_text/1.0.0/9969e1448f410fe7c6c688a84bfcb61312d0a3f2741d57341c26ef99f28a5451)\n"
     ]
    }
   ],
   "source": [
    "nli_data = datasets.load_dataset(\"multi_nli\")\n",
    "\n",
    "train_dataset = nli_data['train'].select(range(20000)) \n",
    "# limiting the training set size to 20,000 for demo purposes\n",
    "dev_dataset = nli_data['validation_matched']\n",
    "test_dataset = nli_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "practical-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Workers carve sculptures and paint scrolls with great enthusiasm.\n",
      "Premise   :  The individual artisans' shops are no longer here, but you can visit a silk-weaving factory, a ceramics plant, and the Foshan Folk Art Studio, where you can observe workers making Chinese lanterns, carving sculptures, painting scrolls, and cutting intricate designs in paper.\n",
      "Label     :  neutral\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Sir Ernest bent his head slightly, and continued.\n",
      "Premise   :  Really, Sir Ernest, protested the judge, \"these questions are not relevant.\" Sir Ernest bowed, and having shot his arrow proceeded. \n",
      "Label     :  entailment\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  The house is very large and boasts over ten bedrooms, a huge kitchen, and a full sized olympic pool.\n",
      "Premise   :  The house is surprisingly small and simple, with one bedroom, a tiny kitchen, and a couple of social rooms.\n",
      "Label     :  contradiction\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Hiding things is just dirty, whereas there is glory in fiction\n",
      "Premise   :  Fiction has its glories, but concealment is merely squalid.\n",
      "Label     :  entailment\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  It would take some time before high-risk people start paying a premium.\n",
      "Premise   :  um but i think in time come you're going to see that happening you're going to see um where the high risk people pay a premium but they have to find a way to prove it\n",
      "Label     :  neutral\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  No children ever want to be entertained.\n",
      "Premise   :  Are there children who need to be entertained?\n",
      "Label     :  contradiction\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Wall Street is facing issues, that need to be addressed. \n",
      "Premise   :  Hopefully, Wall Street will take voluntary steps to address these issues before it is forced to act.\n",
      "Label     :  entailment\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  The New York Giants and Raiders are my favorite teams in football.\n",
      "Premise   :  okay pro football i like two teams one the New York Giants and the second is the Raiders\n",
      "Label     :  entailment\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  The doctors office was a terrible place and he wanted out.\n",
      "Premise   :  He'd stopped wondering and now accepted; he meant to get away from here at the first chance and he was somehow sure he could.\n",
      "Label     :  neutral\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  i'm a taste person and all vegetables taste very nice unlike other types of food\n",
      "Premise   :  yeah it's strange because well it it's not strange because i use to be the same way and i'm even to this day  you know some vegetables really turn me off but when you read so much information that says this is a healthier way to go you know and this is what your body wants this is what your body really needs and when you think about what is what's the real reason your eating i know i know it's for taste because i'm boy am i a taste person but\n",
      "Label     :  contradiction\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  in public places there are absolutely no place that does that\n",
      "Premise   :  in public places there is one state that does that by the way\n",
      "Label     :  contradiction\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Due to inflation and the production value, Las Vegas shows now cost more to see.\n",
      "Premise   :  Once, Las Vegas showrooms were filled with top entertainment headliners, comedians, production shows, and dancing girls that could be enjoyed at a very low price.\n",
      "Label     :  neutral\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  It wouldn't cost them much money to just stick on those things.\n",
      "Premise   :  uh stick on those things and they can just all that's all they have to do i mean that wouldn't cost a great deal of money and uh\n",
      "Label     :  entailment\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  In addition to bathing in streams, we've also gone to spas and saunas.\n",
      "Premise   :  we have gone on trips we've bathed in streams\n",
      "Label     :  neutral\n",
      "================================================================================ \n",
      "\n",
      "Hypothesis:  Philip II of Spain invaded Portugal.\n",
      "Premise   :  On the mainland, an invasion of even greater significance followed in 1580, when Philip II of Spain proclaimed himself king of Portugal and marched his armies across the border.\n",
      "Label     :  entailment\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "\n",
    "# Clearer if it is a dict\n",
    "label_name = {\n",
    "    0: \"entailment\",\n",
    "    1: \"neutral\", \n",
    "    2: \"contradiction\",\n",
    "}\n",
    "\n",
    "for num in range(170, 185):\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "    print(\"Hypothesis: \", train_dataset[\"hypothesis\"][num])\n",
    "    print(\"Premise   : \", train_dataset[\"premise\"][num])\n",
    "    print(\"Label     : \", label_name[train_dataset[\"label\"][num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "developing-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450f8b5af7b44552a303c20f68ed262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be61d58128c0433daf0e0c29e9201f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.model_max_length = 256\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['premise'], batch['hypothesis'], padding='max_length', truncation=True)\n",
    "\n",
    "batch_size = 16\n",
    "eval_batch_size = 8\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-garage",
   "metadata": {},
   "source": [
    "You can check the tokenizer has added a `[SEP]` token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pediatric-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = {sum(example[\"attention_mask\"]) for example in train_dataset}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hungry-capitol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "worth-forest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([l for l in lens if l >= 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-invasion",
   "metadata": {},
   "source": [
    "Uso 256!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suitable-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['attention_mask', 'hypothesis', 'input_ids', 'label', 'premise'])\n",
      "Premise    : Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Hypothesis : Product and geography are what make cream skimming work. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "\n",
    "print(example.keys())\n",
    "print(\"Premise    :\", example[\"premise\"])\n",
    "print(\"Hypothesis :\", example[\"hypothesis\"])\n",
    "\n",
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "transsexual-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "declared-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for Trainer\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    #_, _, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        #'macro f1': macro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pursuant-departure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 25:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.796600</td>\n",
       "      <td>0.719308</td>\n",
       "      <td>0.688130</td>\n",
       "      <td>0.686414</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>0.686998</td>\n",
       "      <td>40.642500</td>\n",
       "      <td>241.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>0.695121</td>\n",
       "      <td>0.707590</td>\n",
       "      <td>0.707682</td>\n",
       "      <td>0.710948</td>\n",
       "      <td>0.708051</td>\n",
       "      <td>40.654400</td>\n",
       "      <td>241.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>0.871913</td>\n",
       "      <td>0.719715</td>\n",
       "      <td>0.717914</td>\n",
       "      <td>0.718131</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>40.474300</td>\n",
       "      <td>242.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>1.372733</td>\n",
       "      <td>0.722364</td>\n",
       "      <td>0.719126</td>\n",
       "      <td>0.721440</td>\n",
       "      <td>0.720013</td>\n",
       "      <td>40.574300</td>\n",
       "      <td>241.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>1.580839</td>\n",
       "      <td>0.724401</td>\n",
       "      <td>0.723247</td>\n",
       "      <td>0.723330</td>\n",
       "      <td>0.723287</td>\n",
       "      <td>40.869200</td>\n",
       "      <td>240.156000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.38396575485229495, metrics={'train_runtime': 1529.4957, 'train_samples_per_second': 4.086, 'total_flos': 10284407654400000, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "epochs = 5\n",
    "\n",
    "total_steps = (epochs * len(train_dataset)) // batch_size\n",
    "warmup_steps = total_steps // 10\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "smoking-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1227' max='1227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1227/1227 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.580838918685913,\n",
       " 'eval_accuracy': 0.7244014263881814,\n",
       " 'eval_f1': 0.7232469122677775,\n",
       " 'eval_precision': 0.7233302559520883,\n",
       " 'eval_recall': 0.723286715463998,\n",
       " 'eval_runtime': 36.6277,\n",
       " 'eval_samples_per_second': 267.966,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dev_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
