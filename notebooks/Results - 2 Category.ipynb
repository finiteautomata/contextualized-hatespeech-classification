{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "found-local",
   "metadata": {},
   "source": [
    "## Task B: Category Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embedded-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "protecting-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "We have 9 full context evaluations\n",
      "We have 16 context evaluations\n",
      "We have 16 no context evaluations\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hatedetection import load_datasets\n",
    "import glob\n",
    "import json\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load_datasets(add_body=True)\n",
    "\n",
    "no_context_evals = []\n",
    "context_evals = []\n",
    "full_context_evals = []\n",
    "\n",
    "for path in glob.glob(\"../evaluations/non-context-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        no_context_evals.append(obj)\n",
    "\n",
    "for path in glob.glob(\"../evaluations/context-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        context_evals.append(obj)\n",
    "\n",
    "\n",
    "for path in glob.glob(\"../evaluations/title-body-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        full_context_evals.append(obj)\n",
    "\n",
    "print(f\"We have {len(full_context_evals)} full context evaluations\")\n",
    "print(f\"We have {len(context_evals)} context evaluations\")\n",
    "print(f\"We have {len(no_context_evals)} no context evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compressed-steel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full context mean</th>\n",
       "      <th>full context std</th>\n",
       "      <th>context mean</th>\n",
       "      <th>context std</th>\n",
       "      <th>no context mean</th>\n",
       "      <th>no context std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_calls_f1</th>\n",
       "      <td>0.801411</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.801637</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.784165</td>\n",
       "      <td>0.008949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_women_f1</th>\n",
       "      <td>0.713182</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.672225</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.652158</td>\n",
       "      <td>0.010933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_lgbti_f1</th>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>0.842527</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>0.590471</td>\n",
       "      <td>0.017874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_racism_f1</th>\n",
       "      <td>0.939435</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.942906</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.862699</td>\n",
       "      <td>0.004899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_class_f1</th>\n",
       "      <td>0.738182</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.726768</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.593249</td>\n",
       "      <td>0.011060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_politics_f1</th>\n",
       "      <td>0.777379</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.752979</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>0.717805</td>\n",
       "      <td>0.014555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_disabled_f1</th>\n",
       "      <td>0.793302</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.750196</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.786369</td>\n",
       "      <td>0.015904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_appearance_f1</th>\n",
       "      <td>0.890847</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.878756</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.844621</td>\n",
       "      <td>0.004387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_criminal_f1</th>\n",
       "      <td>0.910827</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.901019</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.744453</td>\n",
       "      <td>0.008515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_f1</th>\n",
       "      <td>0.824928</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.807668</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.730666</td>\n",
       "      <td>0.003954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_precision</th>\n",
       "      <td>0.852357</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.785975</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_recall</th>\n",
       "      <td>0.801152</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.769902</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.005748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     full context mean  full context std  context mean  \\\n",
       "eval_calls_f1                 0.801411          0.019641      0.801637   \n",
       "eval_women_f1                 0.713182          0.011052      0.672225   \n",
       "eval_lgbti_f1                 0.859784          0.010172      0.842527   \n",
       "eval_racism_f1                0.939435          0.009133      0.942906   \n",
       "eval_class_f1                 0.738182          0.015776      0.726768   \n",
       "eval_politics_f1              0.777379          0.008716      0.752979   \n",
       "eval_disabled_f1              0.793302          0.014691      0.750196   \n",
       "eval_appearance_f1            0.890847          0.011963      0.878756   \n",
       "eval_criminal_f1              0.910827          0.004219      0.901019   \n",
       "eval_mean_f1                  0.824928          0.005597      0.807668   \n",
       "eval_mean_precision           0.852357          0.006309      0.853147   \n",
       "eval_mean_recall              0.801152          0.006463      0.769902   \n",
       "\n",
       "                     context std  no context mean  no context std  \n",
       "eval_calls_f1           0.009916         0.784165        0.008949  \n",
       "eval_women_f1           0.014997         0.652158        0.010933  \n",
       "eval_lgbti_f1           0.020611         0.590471        0.017874  \n",
       "eval_racism_f1          0.004489         0.862699        0.004899  \n",
       "eval_class_f1           0.012011         0.593249        0.011060  \n",
       "eval_politics_f1        0.007360         0.717805        0.014555  \n",
       "eval_disabled_f1        0.028966         0.786369        0.015904  \n",
       "eval_appearance_f1      0.009601         0.844621        0.004387  \n",
       "eval_criminal_f1        0.007573         0.744453        0.008515  \n",
       "eval_mean_f1            0.006250         0.730666        0.003954  \n",
       "eval_mean_precision     0.006796         0.785975        0.004333  \n",
       "eval_mean_recall        0.008000         0.686905        0.005748  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metric_columns = [\n",
    "    'eval_calls_f1', 'eval_women_f1', 'eval_lgbti_f1', 'eval_racism_f1',\n",
    "    'eval_class_f1', 'eval_politics_f1', 'eval_disabled_f1',\n",
    "    'eval_appearance_f1', 'eval_criminal_f1', 'eval_mean_f1',\n",
    "    'eval_mean_precision', 'eval_mean_recall'    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df_full_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in full_context_evals\n",
    "])\n",
    "\n",
    "df_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in context_evals\n",
    "])\n",
    "\n",
    "df_no_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in no_context_evals\n",
    "])\n",
    "\n",
    "full_context_df = pd.DataFrame({\n",
    "    \"full context mean\": df_full_context_evals[metric_columns].mean(), \n",
    "    \"full context std\": df_full_context_evals[metric_columns].std()})\n",
    "\n",
    "context_df = pd.DataFrame({\n",
    "    \"context mean\": df_context_evals[metric_columns].mean(), \n",
    "    \"context std\": df_context_evals[metric_columns].std()\n",
    "})\n",
    "\n",
    "no_context_df = pd.DataFrame({\n",
    "    \"no context mean\": df_no_context_evals[metric_columns].mean(), \n",
    "    \"no context std\": df_no_context_evals[metric_columns].std()})\n",
    "\n",
    "\n",
    "result_df = pd.concat([full_context_df, context_df, no_context_df], axis=1)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "beneficial-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &    full context &         context &      no context \\\\\n",
      "metrics        &                 &                 &                 \\\\\n",
      "\\midrule\n",
      "calls f1       &  0.801 +- 0.020 &  0.802 +- 0.010 &  0.784 +- 0.009 \\\\\n",
      "women f1       &  0.713 +- 0.011 &  0.672 +- 0.015 &  0.652 +- 0.011 \\\\\n",
      "lgbti f1       &  0.860 +- 0.010 &  0.843 +- 0.021 &  0.590 +- 0.018 \\\\\n",
      "racism f1      &  0.939 +- 0.009 &  0.943 +- 0.004 &  0.863 +- 0.005 \\\\\n",
      "class f1       &  0.738 +- 0.016 &  0.727 +- 0.012 &  0.593 +- 0.011 \\\\\n",
      "politics f1    &  0.777 +- 0.009 &  0.753 +- 0.007 &  0.718 +- 0.015 \\\\\n",
      "disabled f1    &  0.793 +- 0.015 &  0.750 +- 0.029 &  0.786 +- 0.016 \\\\\n",
      "appearance f1  &  0.891 +- 0.012 &  0.879 +- 0.010 &  0.845 +- 0.004 \\\\\n",
      "criminal f1    &  0.911 +- 0.004 &  0.901 +- 0.008 &  0.744 +- 0.009 \\\\\n",
      "mean f1        &  0.825 +- 0.006 &  0.808 +- 0.006 &  0.731 +- 0.004 \\\\\n",
      "mean precision &  0.852 +- 0.006 &  0.853 +- 0.007 &  0.786 +- 0.004 \\\\\n",
      "mean recall    &  0.801 +- 0.006 &  0.770 +- 0.008 &  0.687 +- 0.006 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_mean(row, context, ):\n",
    "    mean = row[context + \" mean\"]\n",
    "    std = row[context + \" std\"]\n",
    "    return f\"{mean:.3f} +- {std:.3f}\"\n",
    "\n",
    "display_df = pd.DataFrame()\n",
    "\n",
    "for context in [\"full context\", \"context\", \"no context\"]:\n",
    "    display_df[context] = result_df.apply(lambda x: print_mean(x, context), axis=1)\n",
    "\n",
    "\n",
    "display_df[\"metrics\"] = display_df.index.map(lambda x: \" \".join(x.split(\"_\")[1:]))\n",
    "display_df.reset_index(inplace=True)\n",
    "del display_df[\"index\"]\n",
    "display_df.set_index(\"metrics\", inplace=True)\n",
    "print(display_df.to_latex(escape=False, longtable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-ambassador",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "peaceful-spirituality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full context mean    0.801411\n",
      "full context std     0.019641\n",
      "context mean         0.801637\n",
      "context std          0.009916\n",
      "no context mean      0.784165\n",
      "no context std       0.008949\n",
      "Name: eval_calls_f1, dtype: float64\n",
      "full context mean    0.713182\n",
      "full context std     0.011052\n",
      "context mean         0.672225\n",
      "context std          0.014997\n",
      "no context mean      0.652158\n",
      "no context std       0.010933\n",
      "Name: eval_women_f1, dtype: float64\n",
      "full context mean    0.859784\n",
      "full context std     0.010172\n",
      "context mean         0.842527\n",
      "context std          0.020611\n",
      "no context mean      0.590471\n",
      "no context std       0.017874\n",
      "Name: eval_lgbti_f1, dtype: float64\n",
      "full context mean    0.939435\n",
      "full context std     0.009133\n",
      "context mean         0.942906\n",
      "context std          0.004489\n",
      "no context mean      0.862699\n",
      "no context std       0.004899\n",
      "Name: eval_racism_f1, dtype: float64\n",
      "full context mean    0.738182\n",
      "full context std     0.015776\n",
      "context mean         0.726768\n",
      "context std          0.012011\n",
      "no context mean      0.593249\n",
      "no context std       0.011060\n",
      "Name: eval_class_f1, dtype: float64\n",
      "full context mean    0.777379\n",
      "full context std     0.008716\n",
      "context mean         0.752979\n",
      "context std          0.007360\n",
      "no context mean      0.717805\n",
      "no context std       0.014555\n",
      "Name: eval_politics_f1, dtype: float64\n",
      "full context mean    0.793302\n",
      "full context std     0.014691\n",
      "context mean         0.750196\n",
      "context std          0.028966\n",
      "no context mean      0.786369\n",
      "no context std       0.015904\n",
      "Name: eval_disabled_f1, dtype: float64\n",
      "full context mean    0.890847\n",
      "full context std     0.011963\n",
      "context mean         0.878756\n",
      "context std          0.009601\n",
      "no context mean      0.844621\n",
      "no context std       0.004387\n",
      "Name: eval_appearance_f1, dtype: float64\n",
      "full context mean    0.910827\n",
      "full context std     0.004219\n",
      "context mean         0.901019\n",
      "context std          0.007573\n",
      "no context mean      0.744453\n",
      "no context std       0.008515\n",
      "Name: eval_criminal_f1, dtype: float64\n",
      "full context mean    0.824928\n",
      "full context std     0.005597\n",
      "context mean         0.807668\n",
      "context std          0.006250\n",
      "no context mean      0.730666\n",
      "no context std       0.003954\n",
      "Name: eval_mean_f1, dtype: float64\n",
      "full context mean    0.852357\n",
      "full context std     0.006309\n",
      "context mean         0.853147\n",
      "context std          0.006796\n",
      "no context mean      0.785975\n",
      "no context std       0.004333\n",
      "Name: eval_mean_precision, dtype: float64\n",
      "full context mean    0.801152\n",
      "full context std     0.006463\n",
      "context mean         0.769902\n",
      "context std          0.008000\n",
      "no context mean      0.686905\n",
      "no context std       0.005748\n",
      "Name: eval_mean_recall, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval_calls_f1          None\n",
       "eval_women_f1          None\n",
       "eval_lgbti_f1          None\n",
       "eval_racism_f1         None\n",
       "eval_class_f1          None\n",
       "eval_politics_f1       None\n",
       "eval_disabled_f1       None\n",
       "eval_appearance_f1     None\n",
       "eval_criminal_f1       None\n",
       "eval_mean_f1           None\n",
       "eval_mean_precision    None\n",
       "eval_mean_recall       None\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.apply(lambda row: print(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "color-decade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  full context mean &  full context std &  context mean &  context std &  no context mean &  no context std \\\\\n",
      "\\midrule\n",
      "eval\\_calls\\_f1       &           0.801411 &          0.019641 &      0.801637 &     0.009916 &         0.784165 &        0.008949 \\\\\n",
      "eval\\_women\\_f1       &           0.713182 &          0.011052 &      0.672225 &     0.014997 &         0.652158 &        0.010933 \\\\\n",
      "eval\\_lgbti\\_f1       &           0.859784 &          0.010172 &      0.842527 &     0.020611 &         0.590471 &        0.017874 \\\\\n",
      "eval\\_racism\\_f1      &           0.939435 &          0.009133 &      0.942906 &     0.004489 &         0.862699 &        0.004899 \\\\\n",
      "eval\\_class\\_f1       &           0.738182 &          0.015776 &      0.726768 &     0.012011 &         0.593249 &        0.011060 \\\\\n",
      "eval\\_politics\\_f1    &           0.777379 &          0.008716 &      0.752979 &     0.007360 &         0.717805 &        0.014555 \\\\\n",
      "eval\\_disabled\\_f1    &           0.793302 &          0.014691 &      0.750196 &     0.028966 &         0.786369 &        0.015904 \\\\\n",
      "eval\\_appearance\\_f1  &           0.890847 &          0.011963 &      0.878756 &     0.009601 &         0.844621 &        0.004387 \\\\\n",
      "eval\\_criminal\\_f1    &           0.910827 &          0.004219 &      0.901019 &     0.007573 &         0.744453 &        0.008515 \\\\\n",
      "eval\\_mean\\_f1        &           0.824928 &          0.005597 &      0.807668 &     0.006250 &         0.730666 &        0.003954 \\\\\n",
      "eval\\_mean\\_precision &           0.852357 &          0.006309 &      0.853147 &     0.006796 &         0.785975 &        0.004333 \\\\\n",
      "eval\\_mean\\_recall    &           0.801152 &          0.006463 &      0.769902 &     0.008000 &         0.686905 &        0.005748 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(result_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-friendly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
