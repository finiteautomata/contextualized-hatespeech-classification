{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "devoted-advertiser",
   "metadata": {},
   "source": [
    "## Task B: Category Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ideal-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superior-retailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "We have 9 full context evaluations\n",
      "We have 16 context evaluations\n",
      "We have 16 no context evaluations\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hatedetection import load_datasets\n",
    "import glob\n",
    "import json\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load_datasets(add_body=True)\n",
    "\n",
    "no_context_evals = []\n",
    "context_evals = []\n",
    "full_context_evals = []\n",
    "\n",
    "for path in glob.glob(\"../evaluations/non-context-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        no_context_evals.append(obj)\n",
    "\n",
    "for path in glob.glob(\"../evaluations/context-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        context_evals.append(obj)\n",
    "\n",
    "\n",
    "for path in glob.glob(\"../evaluations/title-body-category*\"):\n",
    "    with open(path) as f:\n",
    "        obj = json.load(f)\n",
    "        obj[\"file\"] = path\n",
    "        full_context_evals.append(obj)\n",
    "\n",
    "print(f\"We have {len(full_context_evals)} full context evaluations\")\n",
    "print(f\"We have {len(context_evals)} context evaluations\")\n",
    "print(f\"We have {len(no_context_evals)} no context evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "micro-brighton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full context mean</th>\n",
       "      <th>full context std</th>\n",
       "      <th>context mean</th>\n",
       "      <th>context std</th>\n",
       "      <th>no context mean</th>\n",
       "      <th>no context std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_calls_f1</th>\n",
       "      <td>0.801411</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.801637</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.784165</td>\n",
       "      <td>0.008949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_women_f1</th>\n",
       "      <td>0.713182</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.672225</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.652158</td>\n",
       "      <td>0.010933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_lgbti_f1</th>\n",
       "      <td>0.859784</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>0.842527</td>\n",
       "      <td>0.020611</td>\n",
       "      <td>0.590471</td>\n",
       "      <td>0.017874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_racism_f1</th>\n",
       "      <td>0.939435</td>\n",
       "      <td>0.009133</td>\n",
       "      <td>0.942906</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.862699</td>\n",
       "      <td>0.004899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_class_f1</th>\n",
       "      <td>0.738182</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.726768</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.593249</td>\n",
       "      <td>0.011060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_politics_f1</th>\n",
       "      <td>0.777379</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.752979</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>0.717805</td>\n",
       "      <td>0.014555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_disabled_f1</th>\n",
       "      <td>0.793302</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.750196</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.786369</td>\n",
       "      <td>0.015904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_appearance_f1</th>\n",
       "      <td>0.890847</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.878756</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.844621</td>\n",
       "      <td>0.004387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_criminal_f1</th>\n",
       "      <td>0.910827</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.901019</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.744453</td>\n",
       "      <td>0.008515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_f1</th>\n",
       "      <td>0.824928</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.807668</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.730666</td>\n",
       "      <td>0.003954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_precision</th>\n",
       "      <td>0.852357</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.785975</td>\n",
       "      <td>0.004333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_mean_recall</th>\n",
       "      <td>0.801152</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.769902</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.005748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     full context mean  full context std  context mean  \\\n",
       "eval_calls_f1                 0.801411          0.019641      0.801637   \n",
       "eval_women_f1                 0.713182          0.011052      0.672225   \n",
       "eval_lgbti_f1                 0.859784          0.010172      0.842527   \n",
       "eval_racism_f1                0.939435          0.009133      0.942906   \n",
       "eval_class_f1                 0.738182          0.015776      0.726768   \n",
       "eval_politics_f1              0.777379          0.008716      0.752979   \n",
       "eval_disabled_f1              0.793302          0.014691      0.750196   \n",
       "eval_appearance_f1            0.890847          0.011963      0.878756   \n",
       "eval_criminal_f1              0.910827          0.004219      0.901019   \n",
       "eval_mean_f1                  0.824928          0.005597      0.807668   \n",
       "eval_mean_precision           0.852357          0.006309      0.853147   \n",
       "eval_mean_recall              0.801152          0.006463      0.769902   \n",
       "\n",
       "                     context std  no context mean  no context std  \n",
       "eval_calls_f1           0.009916         0.784165        0.008949  \n",
       "eval_women_f1           0.014997         0.652158        0.010933  \n",
       "eval_lgbti_f1           0.020611         0.590471        0.017874  \n",
       "eval_racism_f1          0.004489         0.862699        0.004899  \n",
       "eval_class_f1           0.012011         0.593249        0.011060  \n",
       "eval_politics_f1        0.007360         0.717805        0.014555  \n",
       "eval_disabled_f1        0.028966         0.786369        0.015904  \n",
       "eval_appearance_f1      0.009601         0.844621        0.004387  \n",
       "eval_criminal_f1        0.007573         0.744453        0.008515  \n",
       "eval_mean_f1            0.006250         0.730666        0.003954  \n",
       "eval_mean_precision     0.006796         0.785975        0.004333  \n",
       "eval_mean_recall        0.008000         0.686905        0.005748  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metric_columns = [\n",
    "    'eval_calls_f1', 'eval_women_f1', 'eval_lgbti_f1', 'eval_racism_f1',\n",
    "    'eval_class_f1', 'eval_politics_f1', 'eval_disabled_f1',\n",
    "    'eval_appearance_f1', 'eval_criminal_f1', 'eval_mean_f1',\n",
    "    'eval_mean_precision', 'eval_mean_recall'    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df_full_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in full_context_evals\n",
    "])\n",
    "\n",
    "df_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in context_evals\n",
    "])\n",
    "\n",
    "df_no_context_evals = pd.DataFrame([\n",
    "    {**{\"file\": evaluation[\"file\"]}, **evaluation[\"metrics\"]} for evaluation in no_context_evals\n",
    "])\n",
    "\n",
    "full_context_df = pd.DataFrame({\n",
    "    \"full context mean\": df_full_context_evals[metric_columns].mean(), \n",
    "    \"full context std\": df_full_context_evals[metric_columns].std()})\n",
    "\n",
    "context_df = pd.DataFrame({\n",
    "    \"context mean\": df_context_evals[metric_columns].mean(), \n",
    "    \"context std\": df_context_evals[metric_columns].std()\n",
    "})\n",
    "\n",
    "no_context_df = pd.DataFrame({\n",
    "    \"no context mean\": df_no_context_evals[metric_columns].mean(), \n",
    "    \"no context std\": df_no_context_evals[metric_columns].std()})\n",
    "\n",
    "\n",
    "result_df = pd.concat([full_context_df, context_df, no_context_df], axis=1)\n",
    "\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
