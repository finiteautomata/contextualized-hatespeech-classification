{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain vs finegrained\n",
    "\n",
    "¿Cuánta ganancia tenemos en la detección?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hatedetection import load_datasets\n",
    "import glob\n",
    "import json\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load_datasets(add_body=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs\n",
    "\n",
    "`text` hace referencia al contexto del tweet del diario\n",
    "`title` al título de la noticia (extraído del artículo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beto_fine_body.json\t\t  betonews_plain_none.json\n",
      "beto_fine_none_weighted.json\t  betonews_plain_text.json\n",
      "beto_fine_title.json\t\t  beto_plain_none.json\n",
      "beto_fine_title_nonweighted.json  beto_plain_text+body.json\n",
      "betonews_fine_body.json\t\t  beto_plain_text.json\n",
      "betonews_fine_none.json\t\t  dev\n",
      "betonews_fine_text.json\t\t  old\n",
      "betonews_fine_title.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../evaluations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 plain evaluations\n",
      "We have 10 fine evaluations\n"
     ]
    }
   ],
   "source": [
    "evaluations = {\n",
    "}\n",
    "for model_name, path in [\n",
    "    ('plain', \"../evaluations/betonews_plain_text.json\"),   \n",
    "    ('fine', \"../evaluations/betonews_fine_text.json\"),   \n",
    "    ]:\n",
    "\n",
    "    with open(path) as f:\n",
    "        evaluations[model_name] = json.load(f)\n",
    "\n",
    "for key, evals in evaluations.items():\n",
    "    print(f\"We have {len(evals['metrics'])} {key} evaluations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_loss                    0.615216\n",
       "test_accuracy                0.910138\n",
       "test_f1                      0.697287\n",
       "test_macro_f1                0.822260\n",
       "test_precision               0.748445\n",
       "test_recall                  0.653255\n",
       "test_runtime                33.804930\n",
       "test_samples_per_second    335.544700\n",
       "test_steps_per_second       20.973500\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_plain = pd.DataFrame(evaluations[\"plain\"][\"metrics\"])\n",
    "\n",
    "df_plain.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_loss                    0.068422\n",
       "test_calls_f1                0.684652\n",
       "test_women_f1                0.420590\n",
       "test_lgbti_f1                0.482327\n",
       "test_racism_f1               0.720456\n",
       "test_class_f1                0.511453\n",
       "test_politics_f1             0.624763\n",
       "test_disabled_f1             0.608863\n",
       "test_appearance_f1           0.766254\n",
       "test_criminal_f1             0.699382\n",
       "test_mean_f1                 0.613193\n",
       "test_mean_precision          0.702100\n",
       "test_mean_recall             0.550936\n",
       "test_hate_precision          0.759889\n",
       "test_hate_recall             0.665943\n",
       "test_hate_f1                 0.709637\n",
       "test_runtime                34.542440\n",
       "test_samples_per_second    328.509200\n",
       "test_steps_per_second       10.281300\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fine = pd.DataFrame(evaluations[\"fine\"][\"metrics\"])\n",
    "\n",
    "df_fine.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es más grande en promedio? Hagamos un Mann-Whitney U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=0.0, pvalue=9.133589555477501e-05)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "plain_f1 = df_plain[\"test_f1\"]\n",
    "fine_f1 = df_fine[\"test_hate_f1\"]\n",
    "\n",
    "scipy.stats.mannwhitneyu(plain_f1, fine_f1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1855ee650470317f04d390fdf3db9f02676a933ff89f07f1c362ba1ca836689d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('hatedetection-QvSQzQYL-py3.7': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "3af164d50710d9f221a3ae6c9ac2b1bef73a4fcc4dd97b6430472a2215de3743"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
